{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task KDT04: Text Summarization Using NLP**\n",
        "\n",
        "**Requirements:**\n",
        "-  Use T5, BART, or PEGASUS from Hugging Face\n",
        "-  Train on a small custom dataset (or use CNN/DailyMail)\n",
        "-  Generate summaries for at least 5 sample paragraphs\n",
        "\n",
        "**Bonus:**\n",
        "-  Compare model-generated summaries vs. extractive method (e.g., spaCy)\n",
        "- Include a web form to paste text and view a summary"
      ],
      "metadata": {
        "id": "LxGPW5PYmCvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Required Libraries and Modules"
      ],
      "metadata": {
        "id": "QqwExVQ7mXwa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpxym0UlPFPz"
      },
      "outputs": [],
      "source": [
        "pip install -q datasets sacrebleu evaluate rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets fsspec"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cp95wkIcTwIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollator, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "rAgDB-5oPjYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "lQaBMrHtX0Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# ds = load_dataset(\"cnn_dailymail\", \"1.0.0\", split=\"train[:20000]\")"
      ],
      "metadata": {
        "id": "LVME4umXS_Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ds"
      ],
      "metadata": {
        "id": "rG3x76F5UHRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Multiple Dataset Versions of CNN/DailyMail Dataset from hugging face"
      ],
      "metadata": {
        "id": "IF6safjfmj4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
        "\n",
        "v1 = load_dataset(\"cnn_dailymail\", \"1.0.0\", split=\"train[:20000]\")\n",
        "v2 = load_dataset(\"cnn_dailymail\", \"2.0.0\", split=\"train[:20000]\")\n",
        "v3 = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:20000]\")\n",
        "\n",
        "data = concatenate_datasets([v1, v2, v3])"
      ],
      "metadata": {
        "id": "7Tjgmn-ZP0N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.remove_columns(\"id\")"
      ],
      "metadata": {
        "id": "33GHvfleV6uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "hyTR1Qv-UmGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = data.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "M7D6wkK4UqWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1"
      ],
      "metadata": {
        "id": "wCqyduyDU2wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d1['train']"
      ],
      "metadata": {
        "id": "G1l9ooXuVAxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2 = d1['train'].train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "SFlJ0OW6U4cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2"
      ],
      "metadata": {
        "id": "Ag34DLzWVLo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UskByY3XmvCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Data into Train/Val/Test"
      ],
      "metadata": {
        "id": "f6Uvd8MymwAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = DatasetDict({'train' : d2['train'], 'validation' : d2['test'], 'test' : d1['test']})"
      ],
      "metadata": {
        "id": "FX-JS88DVGw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data"
      ],
      "metadata": {
        "id": "wN2f376oVx7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Pretrained T5 Model & Tokenizer"
      ],
      "metadata": {
        "id": "j2MeLHjImydB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"google-t5/t5-small\")"
      ],
      "metadata": {
        "id": "qFJcLmVfWLT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Function for Tokenization"
      ],
      "metadata": {
        "id": "5FhMxqb_m2CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess(example):\n",
        "    inputs = [\"summarize: \" + article for article in example[\"article\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"highlights\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]\n",
        "        for labels_seq in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = final_data.map(preprocess, batched=True, remove_columns=[\"article\", \"highlights\"])\n"
      ],
      "metadata": {
        "id": "QCR7V6IXuqoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric Computation Setup"
      ],
      "metadata": {
        "id": "rwwrRyQ2m4WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric\n",
        "import evaluate\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels (used as ignore index) with pad_token_id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Strip leading/trailing whitespaces\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # Return the scalar float values directly\n",
        "    return {\n",
        "        \"rouge1\": result[\"rouge1\"] * 100,\n",
        "        \"rouge2\": result[\"rouge2\"] * 100,\n",
        "        \"rougeL\": result[\"rougeL\"] * 100,\n",
        "        \"rougeLsum\": result[\"rougeLsum\"] * 100,\n",
        "    }\n",
        "\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "8TLvNPAVWYzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Arguments and Training the Model"
      ],
      "metadata": {
        "id": "OYi7huMTnCti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./T5-small/results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    report_to=[\"wandb\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    greater_is_better=True,\n",
        "    run_name=\"T5-small-summarization\",\n",
        "    fp16=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "N-wX3SXwYxYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train\n",
        "model = trainer.train()"
      ],
      "metadata": {
        "id": "ZRjp7GAtY9Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Fine-Tuned Model from Latest Checkpoint and Inference on 10 Sample Inputs"
      ],
      "metadata": {
        "id": "by5RH0uonICZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model_path = \"/content/T5-small/results/checkpoint-14400\"  # path to fine-tuned model\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
        "\n",
        "# 10 sample input texts for summarization\n",
        "sample_texts = [\n",
        "    \"summarize: Climate change is accelerating faster than previously predicted. Global temperatures are rising, leading to extreme weather events like hurricanes, droughts, and floods. Scientists urge immediate reduction of greenhouse gas emissions to prevent irreversible damage.\",\n",
        "\n",
        "    \"summarize: The Indian economy grew at 6.1% in the last quarter, driven by strong performance in manufacturing and services. The Reserve Bank of India may consider adjusting interest rates in response to inflation trends and global economic pressures.\",\n",
        "\n",
        "    \"summarize: In the final match of the FIFA World Cup, Argentina beat France in a thrilling penalty shootout. Messi scored twice, securing his legacy as one of the greatest footballers of all time. The match was widely regarded as one of the most exciting in history.\",\n",
        "\n",
        "    \"summarize: Apple's new iPhone 15 introduces a periscope zoom camera, titanium frame, and USB-C port. It also features improvements in battery life and AI-powered photography. Reviews have been largely positive, praising performance and design.\",\n",
        "\n",
        "    \"summarize: A new study shows that a Mediterranean diet, rich in vegetables, olive oil, and fish, can significantly reduce the risk of heart disease. Participants also reported improved mental clarity and energy levels.\",\n",
        "\n",
        "    \"summarize: The Artemis I mission successfully completed its journey around the moon and returned safely to Earth. NASA plans to send astronauts to the moon in the next phase, establishing a long-term human presence as part of the Artemis program.\",\n",
        "\n",
        "    \"summarize: The COVID-19 pandemic exposed vulnerabilities in global healthcare systems. Many countries faced shortages of medical supplies, staff, and ICU beds. Lessons learned have led to new policies aimed at future pandemic preparedness.\",\n",
        "\n",
        "    \"summarize: Elon Musk's acquisition of Twitter sparked both praise and controversy. He implemented mass layoffs, introduced paid verification, and promised more transparency in algorithms. Users and advertisers remain divided over the platform’s direction.\",\n",
        "\n",
        "    \"summarize: Researchers have developed a breakthrough cancer treatment using mRNA technology. Early trials show promising results in targeting tumors with minimal side effects, potentially revolutionizing oncology in the next decade.\",\n",
        "\n",
        "    \"summarize: A massive volcanic eruption in Iceland disrupted air travel across Europe. Ash clouds spread rapidly, grounding flights and affecting millions of travelers. Emergency services were deployed to monitor and manage the situation.\"\n",
        "]\n",
        "\n",
        "# Generate summaries\n",
        "for i, text in enumerate(sample_texts):\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=150,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    print(f\"\\nText {i+1} Summary:\\n{summary}\")\n"
      ],
      "metadata": {
        "id": "AkoUSU2Hb_Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/T5-small/results/checkpoint-14400\")\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"/content/T5-small/results/checkpoint-14400\")"
      ],
      "metadata": {
        "id": "68hKxp4IaKQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate summaries for at least 5 sample paragraphs"
      ],
      "metadata": {
        "id": "5RO8_gz6ncng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "\n",
        "# Load your fine-tuned model and tokenizer\n",
        "model_path = \"/content/T5-small/results/checkpoint-14400\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
        "\n",
        "# 5 long texts\n",
        "long_texts = [\n",
        "    \"summarize: The global climate crisis has reached a critical point, with temperatures rising faster than expected. Scientists have observed a dramatic increase in ice melt in both the Arctic and Antarctic regions. Sea levels are projected to rise significantly by the end of the century, threatening coastal cities worldwide. Forest fires, heatwaves, and hurricanes are becoming more frequent and intense, indicating the need for urgent action. International agreements such as the Paris Accord have called for reducing emissions, but many countries are falling short of their targets. Experts emphasize the need for systemic change in energy production, transportation, and agriculture to curb greenhouse gases effectively.\",\n",
        "\n",
        "    \"summarize: India's digital transformation has accelerated rapidly in the last decade, driven by initiatives like Digital India, UPI, and Aadhaar. The country's fintech sector is now one of the fastest-growing in the world, with millions of users adopting mobile wallets, online banking, and digital lending platforms. Government schemes have helped bring internet access to rural areas, empowering small businesses and farmers through e-commerce and mobile applications. However, challenges like digital literacy, cybersecurity, and data privacy remain significant. Experts suggest that continued investment in infrastructure and policy reform will be essential to sustain this digital revolution and make it inclusive.\",\n",
        "\n",
        "    \"summarize: The Artemis program marks NASA’s ambitious return to lunar exploration. Unlike previous missions, Artemis aims to create a sustainable human presence on the moon. Artemis I successfully tested the Space Launch System and Orion spacecraft in an uncrewed mission around the moon. Artemis II will carry astronauts, and Artemis III plans to land the first woman and person of color on the lunar surface. These missions are stepping stones toward Mars exploration. NASA is collaborating with private partners like SpaceX to develop lunar landers and support systems. Scientists hope to establish a lunar base for research, resource utilization, and testing technologies for deep space travel.\",\n",
        "\n",
        "    \"summarize: Advances in artificial intelligence are reshaping industries across the globe. From natural language processing to computer vision, AI technologies are improving productivity, decision-making, and customer experiences. Healthcare has seen remarkable applications, such as AI-driven diagnostics, personalized treatment plans, and drug discovery. In finance, algorithms detect fraud and automate trading. However, ethical concerns around bias, surveillance, and job displacement are growing. Policymakers are debating regulations to ensure AI is used responsibly. Transparency, accountability, and fairness are crucial to building trust in AI systems. As AI continues to evolve, experts believe human oversight and ethical frameworks will be key to guiding its impact.\",\n",
        "\n",
        "    \"summarize: The COVID-19 pandemic has reshaped how societies function, highlighting the importance of resilience and preparedness. Governments worldwide implemented lockdowns, contact tracing, and mass vaccination to curb the virus's spread. While some countries managed better than others, the pandemic exposed weaknesses in healthcare infrastructure, supply chains, and crisis communication. Remote work, online education, and telemedicine became mainstream, accelerating digital adoption. Researchers developed vaccines at unprecedented speeds using mRNA technology, a breakthrough with potential beyond COVID. Moving forward, experts recommend strengthening health systems, investing in early warning mechanisms, and maintaining global cooperation to face future pandemics more effectively.\"\n",
        "]\n",
        "\n",
        "# Summarization with length comparison\n",
        "for i, text in enumerate(long_texts):\n",
        "    # Tokenize and encode the input\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=150,\n",
        "        min_length=40,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print lengths\n",
        "    original_char_len = len(text)\n",
        "    summary_char_len = len(summary)\n",
        "    original_token_len = len(tokenizer.tokenize(text))\n",
        "    summary_token_len = len(tokenizer.tokenize(summary))\n",
        "\n",
        "    print(f\"\\n--- Long Text {i+1} Summary ---\")\n",
        "    print(f\"Original Text (chars): {original_char_len}, Tokens: {original_token_len}\")\n",
        "    print(f\"Summary Text  (chars): {summary_char_len}, Tokens: {summary_token_len}\")\n",
        "    print(f\"\\nSummary:\\n{summary}\")\n"
      ],
      "metadata": {
        "id": "KqwJJ4nQdABi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying model on HuggingFace"
      ],
      "metadata": {
        "id": "yfOHoOcnnQkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "lh_qJnURbfBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"trohith89/KDTS_T5_Summary_FineTune\")\n",
        "tokenizer.push_to_hub(\"trohith89/KDTS_T5_Summary_FineTune\")"
      ],
      "metadata": {
        "id": "3SaNbfjebxuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\", model=\"trohith89/KDTS_T5_Summary_FineTune\")"
      ],
      "metadata": {
        "id": "wOV-L3EpciKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"summarize: Climate change is accelerating faster than previously predicted. Global temperatures are rising, leading to extreme weather events like hurricanes, droughts, and floods. Scientists urge immediate reduction of greenhouse gas emissions to prevent irreversible damage.\")"
      ],
      "metadata": {
        "id": "4z4kYGLFcxuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"\"\"The Trump administration has ordered US embassies worldwide to immediately stop scheduling visa interviews for foreign students as it prepares to implement comprehensive social media screening for all international applicants.\n",
        "\n",
        "A Tuesday state department cable instructs consular sections to pause adding “any additional student or exchange visitor (F, M, and J) visa appointment capacity until further guidance is issued” within days.\n",
        "\n",
        "The directive, first reported by Politico and now confirmed by the Guardian, could severely delay visa processing and hurt universities – many of which Donald Trump accuses of having far-left ideologies – that rely heavily on foreign students for revenue.\n",
        "\n",
        "“The department is conducting a review of existing operations and processes for screening and vetting of student and exchange visitor visa applicants,” the cable reads. Officials plan to issue guidance on “expanded social media vetting for all such applicants”.\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "_wWddKSadwqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ● Compare model-generated summaries vs. extractive method (e.g., spaCy)\n"
      ],
      "metadata": {
        "id": "OnGViWWGnhc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load T5 model\n",
        "model_path = \"/content/T5-small/results/checkpoint-14400\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
        "\n",
        "# Input texts\n",
        "texts = [\n",
        "    \"summarize: The global climate crisis has reached a critical point, with temperatures rising faster than expected. Scientists have observed a dramatic increase in ice melt in both the Arctic and Antarctic regions. Sea levels are projected to rise significantly by the end of the century, threatening coastal cities worldwide. Forest fires, heatwaves, and hurricanes are becoming more frequent and intense, indicating the need for urgent action. International agreements such as the Paris Accord have called for reducing emissions, but many countries are falling short of their targets. Experts emphasize the need for systemic change in energy production, transportation, and agriculture to curb greenhouse gases effectively.\"\n",
        "]\n",
        "\n",
        "def extractive_summary_spacy(text, top_n=3):\n",
        "    doc = nlp(text)\n",
        "    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "    freq = Counter(words)\n",
        "\n",
        "    # Score sentences based on word frequencies\n",
        "    sent_scores = {}\n",
        "    for sent in doc.sents:\n",
        "        score = sum(freq.get(token.text.lower(), 0) for token in sent if token.is_alpha)\n",
        "        sent_scores[sent] = score\n",
        "\n",
        "    # Get top n sentences\n",
        "    top_sents = sorted(sent_scores, key=sent_scores.get, reverse=True)[:top_n]\n",
        "    return ' '.join([sent.text for sent in top_sents])\n",
        "\n",
        "# Compare summaries\n",
        "for i, text in enumerate(texts):\n",
        "    # Abstractive\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    abstractive_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extractive\n",
        "    original_text = text.replace(\"summarize: \", \"\")\n",
        "    extractive_summary = extractive_summary_spacy(original_text, top_n=3)\n",
        "\n",
        "    # Print\n",
        "    print(f\"\\n--- Text {i+1} Comparison ---\")\n",
        "    print(f\"Original Length: {len(original_text)} chars\")\n",
        "\n",
        "    print(f\"\\nAbstractive Summary:\\n{abstractive_summary}\")\n",
        "    print(f\"Abstractive Length: {len(abstractive_summary)} chars\")\n",
        "\n",
        "    print(f\"\\nExtractive Summary:\\n{extractive_summary}\")\n",
        "    print(f\"Extractive Length: {len(extractive_summary)} chars\")\n"
      ],
      "metadata": {
        "id": "Bxe6hq6CeOrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}